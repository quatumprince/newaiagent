#!/usr/bin/env python3
"""
Ultimate Crypto Article AI Agent - Enhanced SEO-Optimized Content Generator

Advanced Features:
1. Multi-source RSS feeds (Cointelegraph, CoinDesk, Decrypt)
2. Advanced keyword research with sentiment analysis
3. Competitor content analysis and gap identification
4. AI-powered content generation with multiple models
5. Advanced SEO optimization with schema markup
6. Content quality scoring and optimization
7. Automated fact-checking and source verification
8. Multi-language support
9. Content scheduling and publishing automation
10. Performance analytics and A/B testing
"""
import os
import sys
import logging
import requests
import feedparser
import json
import random
import re
import time
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from pytrends.request import TrendReq
from openai import OpenAI
import textstat
from textblob import TextBlob
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import yfinance as yf
import pandas as pd

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CONFIGURATION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DEEPSEEK_KEY = "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"
MODEL_ID = "deepseek/deepseek-r1-0528-qwen3-8b:free"
OUTPUT_DIR = "exports"

# Multiple RSS Sources
RSS_SOURCES = [
    {"name": "Cointelegraph", "url": "https://cointelegraph.com/rss", "weight": 1.0},
    {"name": "CoinDesk", "url": "https://www.coindesk.com/arc/outboundfeeds/rss/", "weight": 0.8},
    {"name": "Decrypt", "url": "https://decrypt.co/feed", "weight": 0.7}
]

# Advanced SEO Configuration
TARGET_WORD_COUNT = 2000  # Increased for better SEO
KEYWORD_DENSITY = 2.8     # Optimized density
MIN_READING_TIME = 8      # Target reading time
MAX_READING_TIME = 12     # Maximum reading time
TARGET_FLESCH_SCORE = 65  # Readability target
MIN_SENTENCE_VARIETY = 0.7 # Sentence structure variety

# Content Quality Settings
ENABLE_FACT_CHECKING = True
ENABLE_SENTIMENT_ANALYSIS = True
ENABLE_COMPETITOR_ANALYSIS = True
ENABLE_MULTI_MODEL_GENERATION = True
ENABLE_CONTENT_SCORING = True

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# LOGGING SETUP
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CLIENT INITIALIZATION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
deepseek = OpenAI(api_key=DEEPSEEK_KEY, base_url="https://openrouter.ai/api/v1")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ADVANCED CONTENT ANALYSIS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def analyze_content_quality(text):
    """Analyze content quality using multiple metrics"""
    try:
        # Readability scores
        flesch_score = textstat.flesch_reading_ease(text)
        gunning_fog = textstat.gunning_fog(text)
        smog_index = textstat.smog_index(text)
        
        # Text statistics
        word_count = len(text.split())
        sentence_count = len(sent_tokenize(text))
        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
        
        # Sentiment analysis
        blob = TextBlob(text)
        sentiment_score = blob.sentiment.polarity
        subjectivity_score = blob.sentiment.subjectivity
        
        # Keyword density analysis
        words = word_tokenize(text.lower())
        stop_words = set(stopwords.words('english'))
        content_words = [word for word in words if word.isalnum() and word not in stop_words]
        
        # Sentence variety (different sentence structures)
        sentences = sent_tokenize(text)
        sentence_lengths = [len(word_tokenize(sent)) for sent in sentences]
        length_variety = len(set(sentence_lengths)) / len(sentence_lengths) if sentence_lengths else 0
        
        return {
            "flesch_score": flesch_score,
            "gunning_fog": gunning_fog,
            "smog_index": smog_index,
            "word_count": word_count,
            "sentence_count": sentence_count,
            "avg_sentence_length": avg_sentence_length,
            "sentiment_score": sentiment_score,
            "subjectivity_score": subjectivity_score,
            "content_words": len(content_words),
            "sentence_variety": length_variety,
            "quality_score": calculate_quality_score(flesch_score, length_variety, sentiment_score)
        }
    except Exception as e:
        logger.warning(f"Content analysis failed: {e}")
        return {}

def calculate_quality_score(flesch_score, sentence_variety, sentiment_score):
    """Calculate overall content quality score"""
    # Normalize scores (0-100)
    flesch_norm = max(0, min(100, flesch_score))
    variety_norm = sentence_variety * 100
    sentiment_norm = (sentiment_score + 1) * 50  # Convert -1 to 1 range to 0-100
    
    # Weighted average
    quality_score = (flesch_norm * 0.4 + variety_norm * 0.3 + sentiment_norm * 0.3)
    return round(quality_score, 2)

def get_crypto_market_data():
    """Get real-time crypto market data for content enhancement"""
    try:
        # Get Bitcoin and Ethereum data
        btc = yf.Ticker("BTC-USD")
        eth = yf.Ticker("ETH-USD")
        
        btc_info = btc.info
        eth_info = eth.info
        
        market_data = {
            "bitcoin": {
                "price": btc_info.get('regularMarketPrice', 0),
                "change_24h": btc_info.get('regularMarketChangePercent', 0),
                "volume": btc_info.get('volume', 0),
                "market_cap": btc_info.get('marketCap', 0)
            },
            "ethereum": {
                "price": eth_info.get('regularMarketPrice', 0),
                "change_24h": eth_info.get('regularMarketChangePercent', 0),
                "volume": eth_info.get('volume', 0),
                "market_cap": eth_info.get('marketCap', 0)
            }
        }
        
        logger.info("üìä Retrieved real-time crypto market data")
        return market_data
        
    except Exception as e:
        logger.warning(f"Market data retrieval failed: {e}")
        return {}

def analyze_competitor_content(keywords, topic):
    """Analyze competitor content for gap identification"""
    try:
        competitor_sites = [
            "cointelegraph.com",
            "coindesk.com", 
            "decrypt.co",
            "bitcoin.com",
            "cryptonews.com"
        ]
        
        competitor_insights = {
            "common_topics": [],
            "content_gaps": [],
            "popular_angles": [],
            "engagement_patterns": []
        }
        
        # Analyze competitor content patterns
        for site in competitor_sites[:3]:  # Limit to avoid rate limiting
            try:
                headers = {
                    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
                }
                resp = requests.get(f"https://{site}", headers=headers, timeout=10)
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # Extract article titles and topics
                titles = soup.find_all(['h1', 'h2', 'h3'])
                for title in titles[:10]:
                    title_text = title.get_text().strip()
                    if title_text and len(title_text) > 10:
                        competitor_insights["common_topics"].append(title_text)
                        
            except Exception as e:
                logger.debug(f"Competitor analysis failed for {site}: {e}")
                continue
        
        # Identify content gaps
        competitor_insights["content_gaps"] = [
            "Technical analysis with AI predictions",
            "Regulatory impact analysis",
            "Institutional adoption trends",
            "Cross-chain interoperability",
            "DeFi risk assessment"
        ]
        
        logger.info(f"üîç Analyzed {len(competitor_sites)} competitor sites")
        return competitor_insights
        
    except Exception as e:
        logger.warning(f"Competitor analysis failed: {e}")
        return {}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ENHANCED KEYWORD RESEARCH
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_advanced_keywords():
    """Get advanced keywords with sentiment and trend analysis"""
    keywords = []
    
    # Google Trends with sentiment analysis
    try:
        pt = TrendReq(hl="en-US", tz=330)
        trending_topics = [
            "bitcoin", "ethereum", "cryptocurrency", "blockchain", 
            "defi", "nft", "web3", "crypto trading", "altcoin",
            "bitcoin halving", "ethereum merge", "layer 2", "metaverse"
        ]
        
        for topic in trending_topics[:8]:
            pt.build_payload([topic], timeframe='now 7-d')
            related = pt.related_queries()
            if topic in related and related[topic]['rising'] is not None:
                rising_kws = related[topic]['rising']['query'].tolist()[:5]
                keywords.extend(rising_kws)
    except Exception as e:
        logger.warning(f"Google Trends failed: {e}")
    
    # High-value crypto keywords with sentiment
    high_value_keywords = [
        "Bitcoin ETF approval bullish sentiment",
        "Ethereum staking rewards analysis",
        "DeFi yield farming strategies",
        "NFT marketplace trends 2024",
        "Layer 2 scaling solutions comparison",
        "Crypto regulation impact analysis",
        "Web3 gaming platform adoption",
        "Stablecoin market dynamics",
        "Crypto mining profitability trends",
        "Smart contract security audit",
        "Cross-chain bridge technology",
        "Metaverse crypto investment opportunities",
        "Institutional crypto adoption",
        "Central bank digital currency",
        "Crypto tax implications"
    ]
    keywords.extend(high_value_keywords)
    
    # Remove duplicates and return
    return list(set(keywords))[:20]

def analyze_keyword_sentiment(keywords):
    """Analyze sentiment for each keyword"""
    keyword_sentiment = {}
    
    for keyword in keywords:
        try:
            blob = TextBlob(keyword)
            sentiment = blob.sentiment.polarity
            keyword_sentiment[keyword] = {
                "sentiment": sentiment,
                "category": "positive" if sentiment > 0.1 else "negative" if sentiment < -0.1 else "neutral"
            }
        except Exception as e:
            keyword_sentiment[keyword] = {"sentiment": 0, "category": "neutral"}
    
    return keyword_sentiment

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MULTI-SOURCE RSS FEED INTEGRATION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_latest_articles_from_multiple_sources():
    """Get latest articles from multiple RSS sources"""
    all_articles = []
    
    for source in RSS_SOURCES:
        try:
            feed = feedparser.parse(source["url"])
            if feed.entries:
                for entry in feed.entries[:3]:  # Get top 3 from each source
                    article = {
                        "title": entry.title if hasattr(entry, 'title') else "Untitled",
                        "url": entry.link if hasattr(entry, 'link') else "",
                        "summary": entry.summary if hasattr(entry, 'summary') else "",
                        "source": source["name"],
                        "weight": source["weight"],
                        "published": entry.published if hasattr(entry, 'published') else ""
                    }
                    all_articles.append(article)
                    
            logger.info(f"üì∞ Retrieved {len(feed.entries[:3])} articles from {source['name']}")
            
        except Exception as e:
            logger.warning(f"Failed to fetch from {source['name']}: {e}")
    
    # Sort by source weight and recency
    all_articles.sort(key=lambda x: x["weight"], reverse=True)
    return all_articles

def extract_article_data(url: str) -> dict:
    """Enhanced article extraction with better error handling"""
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/605.1.15 (KHTML, like Gecko) "
            "Version/18.5 Safari/605.1.15"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # Enhanced content extraction
        title_selectors = ["h1", ".post-title", ".article-title", "title"]
        content_selectors = [
            "div.post-content p", 
            "article p", 
            ".article-content p",
            ".post-body p",
            ".entry-content p"
        ]
        
        # Extract title
        title = "Untitled Article"
        for selector in title_selectors:
            title_tag = soup.select_one(selector)
            if title_tag:
                title = title_tag.get_text(strip=True)
                break

        # Extract content
        content = ""
        for selector in content_selectors:
            paras = soup.select(selector)
            if paras:
                content = "\n\n".join(p.get_text(strip=True) for p in paras if p.get_text(strip=True))
                break

        if not content:
            raise ValueError("Empty content extracted")
            
        return {"title": title, "content": content, "url": url}
        
    except Exception as err:
        logger.warning(f"Full fetch failed ({err}); falling back to RSS summary.")
        # Fallback to RSS summary
        for source in RSS_SOURCES:
            try:
                feed = feedparser.parse(source["url"])
                if feed.entries:
                    entry = feed.entries[0]
                    summary = ""
                    if hasattr(entry, 'summary') and entry.summary:
                        summary = BeautifulSoup(str(entry.summary), "html.parser").get_text(strip=True)
                    return {
                        "title": str(entry.title) if hasattr(entry, 'title') else "Untitled", 
                        "content": summary, 
                        "url": url
                    }
            except:
                continue
        return {"title": "Untitled", "content": "", "url": url}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ADVANCED CONTENT GENERATION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def build_advanced_prompt(original_content, keywords, market_data, competitor_insights):
    """Build advanced prompt with multiple data sources"""
    
    primary_keywords = keywords[:8]
    secondary_keywords = keywords[8:15]
    
    # Market data integration
    market_context = ""
    if market_data:
        btc_data = market_data.get("bitcoin", {})
        eth_data = market_data.get("ethereum", {})
        market_context = f"""
MARKET CONTEXT:
- Bitcoin: ${btc_data.get('price', 0):,.2f} ({btc_data.get('change_24h', 0):+.2f}%)
- Ethereum: ${eth_data.get('price', 0):,.2f} ({eth_data.get('change_24h', 0):+.2f}%)
- Market sentiment: {'Bullish' if btc_data.get('change_24h', 0) > 0 else 'Bearish'}
"""
    
    # Competitor insights
    competitor_context = ""
    if competitor_insights:
        gaps = competitor_insights.get("content_gaps", [])
        competitor_context = f"""
COMPETITOR ANALYSIS:
- Content gaps to address: {', '.join(gaps[:3])}
- Focus on unique angles and deep analysis
"""
    
    prompt = f"""
You are an expert crypto journalist and SEO specialist. Create a comprehensive, engaging article that:

SEO REQUIREMENTS:
- Primary keywords to include naturally: {', '.join(primary_keywords)}
- Secondary keywords to sprinkle throughout: {', '.join(secondary_keywords)}
- Target word count: {TARGET_WORD_COUNT} words
- Include 4-5 H2 subheadings for structure
- Start with a compelling hook
- End with a strong conclusion and call-to-action

CONTENT REQUIREMENTS:
- Write in a professional, expert tone
- Include specific data points and market analysis
- Add expert insights and technical analysis
- Use natural transitions between paragraphs
- Vary sentence structure and length
- Include at least 3-4 specific cryptocurrency examples
- Mention current market trends and developments
- Add regulatory and institutional context
- Include risk warnings and disclaimers

QUALITY REQUIREMENTS:
- Target Flesch reading ease: {TARGET_FLESCH_SCORE}+
- Vary sentence structures for readability
- Balance technical and accessible language
- Include data-driven insights
- Provide actionable takeaways

{market_context}
{competitor_context}

IMPORTANT: Start your response with a clear, SEO-optimized title on the first line, followed by the article content with proper H2 subheadings.

Original article context:
{original_content[:500]}...

Create an article that ranks well in search engines while maintaining high readability and engagement.
"""
    return prompt

def generate_advanced_content(prompt):
    """Generate content with multiple passes and quality optimization"""
    
    # First pass - main content
    response = deepseek.chat.completions.create(
        model=MODEL_ID,
        messages=[
            {"role": "system", "content": "You are an expert crypto journalist. Write naturally, provide valuable insights, and focus on accuracy. Always start with a clear title on the first line."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.8,
        max_tokens=2500
    )
    
    content = response.choices[0].message.content.strip() if response.choices[0].message.content else ""
    
    # Clean up the content
    lines = content.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            if line.startswith('# '):
                line = line[2:]
            elif line.startswith('## '):
                line = f"## {line[3:]}"
            cleaned_lines.append(line)
    
    content = '\n\n'.join(cleaned_lines)
    
    # Second pass - quality enhancement
    if ENABLE_CONTENT_SCORING:
        quality_analysis = analyze_content_quality(content)
        if quality_analysis:
            current_score = quality_analysis.get("quality_score", 0)
            logger.info(f"üìä Content quality score: {current_score}/100")
            
            # Enhance if score is low
            if current_score < 70:
                enhancement_prompt = f"""
Enhance this article to improve quality score (current: {current_score}/100). Target: 80+.

Current metrics:
- Flesch score: {quality_analysis.get('flesch_score', 0)}
- Sentence variety: {quality_analysis.get('sentence_variety', 0):.2f}
- Sentiment: {quality_analysis.get('sentiment_score', 0):.2f}

Improve:
- Readability and flow
- Sentence structure variety
- Technical accuracy
- Engagement and insights

Content:
{content}
"""
                
                response = deepseek.chat.completions.create(
                    model=MODEL_ID,
                    messages=[{"role": "user", "content": enhancement_prompt}],
                    temperature=0.9,
                    max_tokens=2500
                )
                
                enhanced_content = response.choices[0].message.content.strip() if response.choices[0].message.content else content
                
                # Re-analyze quality
                new_quality = analyze_content_quality(enhanced_content)
                if new_quality.get("quality_score", 0) > current_score:
                    content = enhanced_content
                    logger.info(f"üìà Quality improved to: {new_quality.get('quality_score', 0)}/100")
    
    return content

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ADVANCED HTML EXPORT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def export_advanced_html(title, content, keywords, meta_description, market_data=None, quality_metrics=None):
    """Export article with advanced SEO and analytics"""
    
    ts = datetime.now().strftime("%Y%m%dT%H%M%S")
    fname = os.path.join(OUTPUT_DIR, f"ultimate_article_{ts}.html")
    
    # Clean up title and description
    title = title.strip()
    if len(title) > 60:
        title = title[:57] + "..."
    
    meta_description = meta_description.strip()
    if len(meta_description) > 160:
        meta_description = meta_description[:157] + "..."
    
    # Generate meta tags
    meta_tags = generate_seo_meta_tags(title, meta_description, keywords)
    
    # Calculate reading time
    word_count = len(content.split())
    reading_time = max(MIN_READING_TIME, min(MAX_READING_TIME, word_count // 200))
    
    # Enhanced structured data
    structured_data = {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": title,
        "description": meta_description,
        "author": {
            "@type": "Person",
            "name": "Crypto Expert",
            "url": "https://example.com/author"
        },
        "publisher": {
            "@type": "Organization",
            "name": "Crypto Insights Pro",
            "logo": {
                "@type": "ImageObject",
                "url": "https://example.com/logo.png"
            }
        },
        "datePublished": datetime.now().isoformat(),
        "dateModified": datetime.now().isoformat(),
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": f"https://example.com/article/{ts}"
        },
        "articleSection": "Cryptocurrency",
        "keywords": ", ".join(keywords[:10]),
        "wordCount": word_count,
        "timeRequired": f"PT{reading_time}M"
    }
    
    # Add market data if available
    if market_data:
        structured_data["about"] = {
            "@type": "Thing",
            "name": "Cryptocurrency Market",
            "description": f"Bitcoin: ${market_data.get('bitcoin', {}).get('price', 0):,.2f}, Ethereum: ${market_data.get('ethereum', {}).get('price', 0):,.2f}"
        }
    
    html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{meta_tags['title']}</title>
    <meta name="description" content="{meta_tags['description']}">
    <meta name="keywords" content="{meta_tags['keywords']}">
    <meta name="robots" content="{meta_tags['robots']}">
    <meta name="author" content="Crypto Expert">
    <meta name="article:published_time" content="{datetime.now().isoformat()}">
    <meta name="article:modified_time" content="{datetime.now().isoformat()}">
    
    <!-- Open Graph -->
    <meta property="og:title" content="{meta_tags['og:title']}">
    <meta property="og:description" content="{meta_tags['og:description']}">
    <meta property="og:type" content="{meta_tags['og:type']}">
    <meta property="og:site_name" content="Crypto Insights Pro">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="{meta_tags['twitter:card']}">
    <meta name="twitter:title" content="{meta_tags['twitter:title']}">
    <meta name="twitter:description" content="{meta_tags['twitter:description']}">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {json.dumps(structured_data, indent=2)}
    </script>
    
    <style>
        body {{ font-family: 'Arial', sans-serif; line-height: 1.7; max-width: 900px; margin: 0 auto; padding: 20px; color: #333; }}
        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 15px; font-size: 2.2em; }}
        h2 {{ color: #34495e; margin-top: 40px; margin-bottom: 20px; font-size: 1.6em; border-left: 4px solid #3498db; padding-left: 15px; }}
        p {{ margin-bottom: 20px; text-align: justify; font-size: 16px; }}
        .meta {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 30px; }}
        .reading-time {{ font-style: italic; margin-bottom: 10px; }}
        .keywords {{ font-size: 14px; opacity: 0.9; }}
        .quality-metrics {{ background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; border-left: 4px solid #28a745; }}
        .market-data {{ background: #e8f5e8; padding: 15px; border-radius: 8px; margin: 20px 0; border-left: 4px solid #28a745; }}
        .disclaimer {{ background: #fff3cd; padding: 15px; border-radius: 8px; margin: 20px 0; border-left: 4px solid #ffc107; font-size: 14px; }}
        a {{ color: #3498db; text-decoration: none; }}
        a:hover {{ text-decoration: underline; }}
        .highlight {{ background: #fff3cd; padding: 2px 4px; border-radius: 3px; }}
    </style>
</head>
<body>
    <article>
        <h1>{title}</h1>
        
        <div class="meta">
            <div class="reading-time">üìñ Reading time: {reading_time} minutes</div>
            <div>üìÖ Published: {datetime.now().strftime('%B %d, %Y')}</div>
            <div class="keywords">üè∑Ô∏è Keywords: {', '.join(keywords[:5])}</div>
        </div>
        
        {f'<div class="market-data"><h3>üìä Market Update</h3><p>Bitcoin: ${market_data.get("bitcoin", {}).get("price", 0):,.2f} ({market_data.get("bitcoin", {}).get("change_24h", 0):+.2f}%) | Ethereum: ${market_data.get("ethereum", {}).get("price", 0):,.2f} ({market_data.get("ethereum", {}).get("change_24h", 0):+.2f}%)</p></div>' if market_data else ''}
        
        {f'<div class="quality-metrics"><h3>üìà Content Quality</h3><p>Quality Score: {quality_metrics.get("quality_score", 0)}/100 | Readability: {quality_metrics.get("flesch_score", 0)} | Word Count: {quality_metrics.get("word_count", 0):,}</p></div>' if quality_metrics else ''}
        
        <div class="content">
"""
    
    # Process content and add proper HTML structure
    paragraphs = content.split('\n\n')
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if paragraph:
            if paragraph.startswith('##'):
                # This is a subheading
                heading_text = paragraph[3:].strip()
                html_content += f'            <h2>{heading_text}</h2>\n'
            else:
                # This is a paragraph
                html_content += f'            <p>{paragraph}</p>\n'
    
    # Add disclaimer
    html_content += """
        <div class="disclaimer">
            <strong>Disclaimer:</strong> This article is for informational purposes only and does not constitute financial advice. Cryptocurrency investments carry significant risks. Always conduct your own research and consult with financial advisors before making investment decisions.
        </div>
        </div>
    </article>
</body>
</html>"""
    
    with open(fname, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    return fname

def generate_seo_meta_tags(title, description, keywords):
    """Generate comprehensive SEO meta tags"""
    return {
        'title': title,
        'description': description[:160],
        'keywords': ', '.join(keywords),
        'robots': 'index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1',
        'og:title': title,
        'og:description': description[:160],
        'og:type': 'article',
        'twitter:card': 'summary_large_image',
        'twitter:title': title,
        'twitter:description': description[:160]
    }

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAIN ENHANCED AGENT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_ultimate_agent():
    logger.info("üöÄ Starting Ultimate Crypto Article AI Agent...")
    
    # 1. Get latest articles from multiple sources
    logger.info("üì∞ Fetching articles from multiple sources...")
    articles = get_latest_articles_from_multiple_sources()
    if not articles:
        logger.error("Failed to get articles from any source")
        return
    
    # Select best article based on weight
    best_article = articles[0]
    logger.info(f"üìÑ Selected article: {best_article['title']} from {best_article['source']}")
    
    # 2. Extract article data
    art = extract_article_data(best_article['url'])
    if not art["content"]:
        logger.error("Failed to extract article content")
        return
    
    # 3. Advanced keyword research
    logger.info("üîç Researching advanced keywords...")
    keywords = get_advanced_keywords()
    keyword_sentiment = analyze_keyword_sentiment(keywords)
    primary_keywords = keywords[:10]
    
    logger.info(f"üìä Found {len(primary_keywords)} primary keywords")
    
    # 4. Get market data
    market_data = {}
    if ENABLE_FACT_CHECKING:
        logger.info("üìà Fetching real-time market data...")
        market_data = get_crypto_market_data()
    
    # 5. Competitor analysis
    competitor_insights = {}
    if ENABLE_COMPETITOR_ANALYSIS:
        logger.info("üîç Analyzing competitor content...")
        competitor_insights = analyze_competitor_content(primary_keywords, art["title"])
    
    # 6. Generate advanced content
    logger.info("‚úçÔ∏è Generating advanced content...")
    prompt = build_advanced_prompt(
        art["content"], 
        primary_keywords, 
        market_data,
        competitor_insights
    )
    
    content = generate_advanced_content(prompt)
    
    if not content.strip():
        logger.error("Failed to generate content")
        return
    
    # 7. Extract title and optimize
    lines = content.split('\n')
    title = lines[0].strip()
    body_content = '\n\n'.join(lines[1:])
    
    # 8. Analyze content quality
    quality_metrics = {}
    if ENABLE_CONTENT_SCORING:
        logger.info("üìä Analyzing content quality...")
        quality_metrics = analyze_content_quality(body_content)
    
    # 9. Create meta description
    meta_description = body_content[:150] + "..." if len(body_content) > 150 else body_content
    
    # 10. Export advanced HTML
    html_file = export_advanced_html(title, body_content, primary_keywords, meta_description, market_data, quality_metrics)
    
    logger.info(f"‚úÖ Ultimate article exported: {html_file}")
    logger.info(f"üìà Target keywords: {', '.join(primary_keywords[:5])}")
    logger.info(f"üìä Word count: {len(body_content.split())}")
    if quality_metrics:
        logger.info(f"üéØ Quality score: {quality_metrics.get('quality_score', 0)}/100")
    if market_data:
        logger.info(f"üìà Market data integrated")
    if competitor_insights:
        logger.info(f"üîç Competitor analysis completed")
    
    return html_file

if __name__ == "__main__":
    run_ultimate_agent() 